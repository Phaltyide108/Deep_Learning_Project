{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get install git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T16:49:28.874225Z","iopub.execute_input":"2025-02-26T16:49:28.874404Z","iopub.status.idle":"2025-02-26T16:49:38.497015Z","shell.execute_reply.started":"2025-02-26T16:49:28.874385Z","shell.execute_reply":"2025-02-26T16:49:38.495973Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSuggested packages:\n  gettext-base git-daemon-run | git-daemon-sysvinit git-doc git-email git-gui gitk gitweb git-cvs\n  git-mediawiki git-svn\nThe following packages will be upgraded:\n  git\n1 upgraded, 0 newly installed, 0 to remove and 116 not upgraded.\nNeed to get 3,165 kB of archives.\nAfter this operation, 4,096 B of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 git amd64 1:2.34.1-1ubuntu1.12 [3,165 kB]\nFetched 3,165 kB in 1s (2,769 kB/s)\n(Reading database ... 127400 files and directories currently installed.)\nPreparing to unpack .../git_1%3a2.34.1-1ubuntu1.12_amd64.deb ...\nUnpacking git (1:2.34.1-1ubuntu1.12) over (1:2.34.1-1ubuntu1.11) ...\nSetting up git (1:2.34.1-1ubuntu1.12) ...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/Phaltyide108/Deep_Learning_Project\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:16:11.824840Z","iopub.execute_input":"2025-02-26T17:16:11.825171Z","iopub.status.idle":"2025-02-26T17:16:12.214778Z","shell.execute_reply.started":"2025-02-26T17:16:11.825141Z","shell.execute_reply":"2025-02-26T17:16:12.213938Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Deep_Learning_Project'...\nremote: Enumerating objects: 4, done.\u001b[K\nremote: Counting objects: 100% (4/4), done.\u001b[K\nremote: Compressing objects: 100% (4/4), done.\u001b[K\nremote: Total 4 (delta 0), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (4/4), done.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!ls\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:16:14.686212Z","iopub.execute_input":"2025-02-26T17:16:14.686500Z","iopub.status.idle":"2025-02-26T17:16:14.802841Z","shell.execute_reply.started":"2025-02-26T17:16:14.686476Z","shell.execute_reply":"2025-02-26T17:16:14.801881Z"}},"outputs":[{"name":"stdout","text":"Deep_Learning_Project  README.md\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!mv RBC_CLASSIFICATION.ipynb Deep_Learning_Project/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:16:17.005643Z","iopub.execute_input":"2025-02-26T17:16:17.006008Z","iopub.status.idle":"2025-02-26T17:16:17.124763Z","shell.execute_reply.started":"2025-02-26T17:16:17.005978Z","shell.execute_reply":"2025-02-26T17:16:17.123778Z"}},"outputs":[{"name":"stdout","text":"mv: cannot stat 'RBC_CLASSIFICATION.ipynb': No such file or directory\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"%cd Deep_Learning_Project\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:11:36.622132Z","iopub.execute_input":"2025-02-26T17:11:36.622424Z","iopub.status.idle":"2025-02-26T17:11:36.627877Z","shell.execute_reply.started":"2025-02-26T17:11:36.622402Z","shell.execute_reply":"2025-02-26T17:11:36.627082Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Deep_Learning_Project\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!git config --global user.name \"Phaltyide108\"\n!git config --global user.email \"dhairyapatel20021012@gmail.com\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:12:23.665267Z","iopub.execute_input":"2025-02-26T17:12:23.665553Z","iopub.status.idle":"2025-02-26T17:12:23.894981Z","shell.execute_reply.started":"2025-02-26T17:12:23.665532Z","shell.execute_reply":"2025-02-26T17:12:23.894043Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"!git add .\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:12:36.016210Z","iopub.execute_input":"2025-02-26T17:12:36.016538Z","iopub.status.idle":"2025-02-26T17:12:36.134926Z","shell.execute_reply.started":"2025-02-26T17:12:36.016512Z","shell.execute_reply":"2025-02-26T17:12:36.133919Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!git commit -m \"Added project files from Kaggle\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:12:51.990548Z","iopub.execute_input":"2025-02-26T17:12:51.990916Z","iopub.status.idle":"2025-02-26T17:12:52.110134Z","shell.execute_reply.started":"2025-02-26T17:12:51.990885Z","shell.execute_reply":"2025-02-26T17:12:52.109340Z"}},"outputs":[{"name":"stdout","text":"On branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!git push origin main\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:13:08.347738Z","iopub.execute_input":"2025-02-26T17:13:08.348056Z","iopub.status.idle":"2025-02-26T17:14:22.508046Z","shell.execute_reply.started":"2025-02-26T17:13:08.348028Z","shell.execute_reply":"2025-02-26T17:14:22.507040Z"}},"outputs":[{"name":"stdout","text":"Username for 'https://github.com': ^C\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!wget -O dataset.zip \"https://figshare.com/ndownloader/files/45776637\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-21T19:25:31.734868Z","iopub.execute_input":"2025-02-21T19:25:31.735172Z","iopub.status.idle":"2025-02-21T19:27:29.584445Z","shell.execute_reply.started":"2025-02-21T19:25:31.735135Z","shell.execute_reply":"2025-02-21T19:27:29.583499Z"}},"outputs":[{"name":"stdout","text":"--2025-02-21 19:25:31--  https://figshare.com/ndownloader/files/45776637\nResolving figshare.com (figshare.com)... 34.250.97.88, 52.17.65.58, 2a05:d018:1f4:d000:4dd5:a536:6713:5a02, ...\nConnecting to figshare.com (figshare.com)|34.250.97.88|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/45776637/Elsafty_RBCs_for_Classification.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20250221/eu-west-1/s3/aws4_request&X-Amz-Date=20250221T192532Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=3ed807b117797ad9e3971927b97703f5e5fb2f8afc4708e98e0c631e30a7cdcb [following]\n--2025-02-21 19:25:32--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/45776637/Elsafty_RBCs_for_Classification.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20250221/eu-west-1/s3/aws4_request&X-Amz-Date=20250221T192532Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=3ed807b117797ad9e3971927b97703f5e5fb2f8afc4708e98e0c631e30a7cdcb\nResolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.41.251, 52.92.35.8, 3.5.64.246, ...\nConnecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.41.251|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3812830020 (3.6G) [application/zip]\nSaving to: ‘dataset.zip’\n\ndataset.zip         100%[===================>]   3.55G  33.6MB/s    in 1m 57s  \n\n2025-02-21 19:27:29 (31.2 MB/s) - ‘dataset.zip’ saved [3812830020/3812830020]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!unzip dataset.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T19:27:29.588138Z","iopub.execute_input":"2025-02-21T19:27:29.588446Z","iopub.status.idle":"2025-02-21T19:28:04.467419Z","shell.execute_reply.started":"2025-02-21T19:27:29.588420Z","shell.execute_reply":"2025-02-21T19:28:04.466568Z"}},"outputs":[{"name":"stdout","text":"Archive:  dataset.zip\n   creating: Elsafty_RBCs_for_Classification/\n   creating: Elsafty_RBCs_for_Classification/Cropped images/\n  inflating: Elsafty_RBCs_for_Classification/Cropped images/CROPPED - Class 1 - Rounded RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Cropped images/CROPPED - Class 2 - Ovalocytes.zip  \n  inflating: Elsafty_RBCs_for_Classification/Cropped images/CROPPED - Class 3 - Fragmented RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Cropped images/CROPPED - Class 4 - Two Overlapping RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Cropped images/CROPPED - Class 5 - Three Overlapping RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Cropped images/CROPPED - Class 6 - Burr Cells.zip  \n  inflating: Elsafty_RBCs_for_Classification/Cropped images/CROPPED - Class 7 - Teardrops.zip  \n  inflating: Elsafty_RBCs_for_Classification/Cropped images/CROPPED - Class 8 - Angled Cells.zip  \n  inflating: Elsafty_RBCs_for_Classification/Cropped images/CROPPED - Class 9 - Borderline Ovalocytes.zip  \n   creating: Elsafty_RBCs_for_Classification/Masks/\n  inflating: Elsafty_RBCs_for_Classification/Masks/Masks - Class 1 - Rounded RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Masks/Masks - Class 2 - Ovalocytes.zip  \n  inflating: Elsafty_RBCs_for_Classification/Masks/Masks - Class 3 - Fragmented RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Masks/Masks - Class 4 - Two Overlapping RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Masks/Masks - Class 5 - Three Overlapping RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Masks/Masks - Class 6 - Burr Cells.zip  \n  inflating: Elsafty_RBCs_for_Classification/Masks/Masks - Class 7 - Teardrops.zip  \n  inflating: Elsafty_RBCs_for_Classification/Masks/Masks - Class 8 - Angled Cells.zip  \n  inflating: Elsafty_RBCs_for_Classification/Masks/Masks - Class 9 - Borderline Ovalocytes.zip  \n   creating: Elsafty_RBCs_for_Classification/Segmented images/\n  inflating: Elsafty_RBCs_for_Classification/Segmented images/SEGMENTED - Class 1 - Rounded RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Segmented images/SEGMENTED - Class 2 - Ovalocytes.zip  \n  inflating: Elsafty_RBCs_for_Classification/Segmented images/SEGMENTED - Class 3 - Fragmented RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Segmented images/SEGMENTED - Class 4 - Two Overlapping RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Segmented images/SEGMENTED - Class 5 - Three Overlapping RBCs.zip  \n  inflating: Elsafty_RBCs_for_Classification/Segmented images/SEGMENTED - Class 6 - Burr Cells.zip  \n  inflating: Elsafty_RBCs_for_Classification/Segmented images/SEGMENTED - Class 7 - Teardrops.zip  \n  inflating: Elsafty_RBCs_for_Classification/Segmented images/SEGMENTED - Class 8 - Angled Cells.zip  \n  inflating: Elsafty_RBCs_for_Classification/Segmented images/SEGMENTED - Class 9 - Borderline Ovalocytes.zip  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport zipfile\nfrom glob import glob\nimport shutil\n\n# Define dataset paths\n# In Kaggle, datasets are usually mounted under `/kaggle/input/`\nzip_folder = \"/kaggle/working/Elsafty_RBCs_for_Classification/Segmented images\"  # Update this path to match your dataset location\nextract_path = \"/kaggle/working/Main_RBC\"  # Use `/kaggle/working/` for saving extracted files\n\n# Step 1: Extract all ZIP files\nos.makedirs(extract_path, exist_ok=True)\nzip_files = glob(os.path.join(zip_folder, \"*.zip\"))\n\nfor zip_file in zip_files:\n    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n        zip_ref.extractall(extract_path)\n\nprint(\"✅ Extraction complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T19:29:20.847209Z","iopub.execute_input":"2025-02-21T19:29:20.847587Z","iopub.status.idle":"2025-02-21T19:30:12.864620Z","shell.execute_reply.started":"2025-02-21T19:29:20.847556Z","shell.execute_reply":"2025-02-21T19:30:12.863851Z"}},"outputs":[{"name":"stdout","text":"✅ Extraction complete!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Step 2: Organize the dataset into 'train', 'val', 'test' folders\ndataset_path = \"/kaggle/working/organized_dataset\"  # Use `/kaggle/working/` for saving organized dataset\ntrain_path = os.path.join(dataset_path, \"train\")\nval_path = os.path.join(dataset_path, \"val\")\ntest_path = os.path.join(dataset_path, \"test\")\n\n# Create directories if they don't exist\nfor split in [train_path, val_path, test_path]:\n    os.makedirs(split, exist_ok=True)\n\n# Move images to structured folders\nall_classes = os.listdir(extract_path)  # `extract_path` is from the previous step\n\n# Example: Move files into train, val, and test folders\n# You need to define your logic for splitting the data (e.g., 70% train, 20% val, 10% test)\nimport random\nrandom.seed(42)  # For reproducibility\n\nfor class_name in all_classes:\n    class_path = os.path.join(extract_path, class_name)\n    images = os.listdir(class_path)\n    random.shuffle(images)  # Shuffle the list of images\n\n    # Define split ratios\n    train_split = int(0.7 * len(images))  # 70% for training\n    val_split = int(0.2 * len(images))   # 20% for validation\n    # Remaining 10% will be for testing\n\n    # Move images to respective folders\n    for i, image in enumerate(images):\n        src = os.path.join(class_path, image)\n        if i < train_split:\n            dst = os.path.join(train_path, class_name, image)\n        elif i < train_split + val_split:\n            dst = os.path.join(val_path, class_name, image)\n        else:\n            dst = os.path.join(test_path, class_name, image)\n\n        # Create class-specific directories if they don't exist\n        os.makedirs(os.path.dirname(dst), exist_ok=True)\n        shutil.move(src, dst)\n\nprint(\"✅ Dataset organized into train, val, and test folders!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T19:30:12.865624Z","iopub.execute_input":"2025-02-21T19:30:12.865922Z","iopub.status.idle":"2025-02-21T19:30:26.170688Z","shell.execute_reply.started":"2025-02-21T19:30:12.865892Z","shell.execute_reply":"2025-02-21T19:30:26.169929Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset organized into train, val, and test folders!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport shutil\nfrom glob import glob\n\n# Define paths\nextract_path = \"/kaggle/working/Main_RBC\"  # Path where files were extracted\ndataset_path = \"/kaggle/working/organized_dataset\"  # Path to save organized dataset\ntrain_path = os.path.join(dataset_path, \"train\")\nval_path = os.path.join(dataset_path, \"val\")\ntest_path = os.path.join(dataset_path, \"test\")\n\n# Create train, val, and test directories\nos.makedirs(train_path, exist_ok=True)\nos.makedirs(val_path, exist_ok=True)\nos.makedirs(test_path, exist_ok=True)\n\n# Organize dataset into train, val, and test folders\nall_classes = os.listdir(extract_path)  # List all class folders in the extracted path\n\nfor class_name in all_classes:\n    class_folder = os.path.join(extract_path, class_name)  # Path to class folder inside extracted data\n    print(class_folder)\n    if not os.path.exists(class_folder):\n        continue  # Skip if the path doesn't exist\n\n    images = glob(os.path.join(class_folder, \"*.png\"))  # Change to \"*.jpg\" if images are in JPG format\n    class_train = os.path.join(train_path, class_name)\n    class_val = os.path.join(val_path, class_name)\n    class_test = os.path.join(test_path, class_name)\n\n    os.makedirs(class_train, exist_ok=True)\n    os.makedirs(class_val, exist_ok=True)\n    os.makedirs(class_test, exist_ok=True)\n\n    # Split dataset (80% train, 10% val, 10% test)\n    num_images = len(images)\n    train_split = int(0.8 * num_images)\n    val_split = int(0.9 * num_images)\n\n    for i, img in enumerate(images):\n        if i < train_split:\n            shutil.move(img, class_train)\n        elif i < val_split:\n            shutil.move(img, class_val)\n        else:\n            shutil.move(img, class_test)\n\nprint(\"✅ Dataset organized into train/val/test sets!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T19:40:11.504276Z","iopub.execute_input":"2025-02-21T19:40:11.504635Z","iopub.status.idle":"2025-02-21T19:40:11.520735Z","shell.execute_reply.started":"2025-02-21T19:40:11.504612Z","shell.execute_reply":"2025-02-21T19:40:11.519869Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Main_RBC/SEGMENTED - Class 2 - Ovalocytes\n/kaggle/working/Main_RBC/SEGMENTED - Class 9 - Borderline Ovalocytes\n/kaggle/working/Main_RBC/SEGMENTED - Class 3 - Fragmented RBCs\n/kaggle/working/Main_RBC/SEGMENTED - Class 5 - Three Overlapping RBCs\n/kaggle/working/Main_RBC/SEGMENTED - Class 7 - Teardrops\n/kaggle/working/Main_RBC/SEGMENTED - Class 8 - Angled Cells\n/kaggle/working/Main_RBC/SEGMENTED - Class 1 - Rounded RBCs\n/kaggle/working/Main_RBC/SEGMENTED - Class 4 - Two Overlapping RBCs\n/kaggle/working/Main_RBC/SEGMENTED - Class 6 - Burr Cells\n✅ Dataset organized into train/val/test sets!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# # Mount drive (if using Colab)\n# from google.colab import drive\n# drive.mount('/content/drive', force_remount=True)\n\n!pip install xlsxwriter\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport shutil\nimport zipfile\nimport random\nfrom glob import glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.applications import EfficientNetB0\n\n########################################################################################################\n##########  DATA ORGANIZATION (Assumed Already Done)\n########################################################################################################\n# Assume that your dataset has been organized into:\n#   - /content/Decompressed_dataset/<class_name>/   (extracted from the ZIPs)\n# And then split (without shuffling) into three directories:\n#   - training_directory, val_directory, test_directory\n# For brevity, we assume that this step is already completed as per your earlier code.\n\ntraining_directory = \"/kaggle/working/organized_dataset/train\"\nval_directory = \"/kaggle/working/organized_dataset/val\"\ntest_directory = \"/kaggle/working/organized_dataset/test\"\n\n########################################################################################################\n##########  CONFIGURATION PARAMETERS (as per paper)\n########################################################################################################\nShuffle_Before_Split = \"no\"       # Not shuffling for better generalization\nselected_split_part = 1\nnum_split_parts = 6\nGive_data_balance = \"no\"\nbatch_size = 32\nEpochs_number = 20\ninitial_LearningRate = 4e-6\nPatience = 3\nmin_delta = 0.01\nimage_width = 80\nimage_height = 80\nnum_classes = len(os.listdir(training_directory))  # Assuming one folder per class\n\n########################################################################################################\n##########  DATA GENERATORS & AUGMENTATION (as used in the paper)\n########################################################################################################\n# Define the target image size (80x80)\nimage_size = (image_height, image_width)\nseed = 123\n\n# Data augmentation for training as in the paper: rotation up to 360, horizontal & vertical flips.\ntrain_datagen = ImageDataGenerator(\n    rotation_range=360,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rescale=1.0/255.0\n)\n\n# For validation and test, only rescaling (no augmentation)\nval_datagen = ImageDataGenerator(rescale=1.0/255.0)\ntest_datagen = ImageDataGenerator(rescale=1.0/255.0)\n\n# Generators\ntrain_generator = train_datagen.flow_from_directory(\n    training_directory,\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='sparse',  # Using sparse labels as in the paper\n    seed=seed\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_directory,\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='sparse',\n    seed=seed\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_directory,\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='sparse',\n    shuffle=False\n)\n\n########################################################################################################\n##########  GPU SETUP & DISTRIBUTION STRATEGY\n########################################################################################################\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Force using GPU 0\n        print(\"GPUs available:\", gpus)\n    except RuntimeError as e:\n        print(e)\nstrategy = tf.distribute.MirroredStrategy()\nprint(\"Number of devices:\", strategy.num_replicas_in_sync)\n\n########################################################################################################\n##########  CLASS WEIGHTS COMPUTATION (using training generator)\n########################################################################################################\nfrom sklearn.utils import class_weight\ntrain_labels = train_generator.classes\nunique_classes = np.unique(train_labels)\nweights = class_weight.compute_class_weight(\n    class_weight='balanced',\n    classes=unique_classes,\n    y=train_labels\n)\nclass_weights_dict = dict(enumerate(weights))\nprint(\"Class weights:\", class_weights_dict)\n\n########################################################################################################\n##########  MODEL ARCHITECTURE: Transfer Learning with EfficientNetB0\n########################################################################################################\n# Use the EfficientNetB0 as the base model\nwith strategy.scope():\n    # Pre-trained model from Keras Applications\n    base_model = EfficientNetB0(\n        weights='imagenet', \n        include_top=False,\n        input_shape=(image_height, image_width, 3)\n    )\n    # In the paper, all layers are set to trainable\n    for layer in base_model.layers:\n        layer.trainable = True\n\n    # Use the EfficientNet preprocessing function\n    preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n    inputs = tf.keras.Input(shape=(image_height, image_width, 3))\n    x = preprocess_input(inputs)\n    x = base_model(x, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.2)(x)\n    outputs = layers.Dense(num_classes)(x)  # No activation, since loss uses logits\n    model = tf.keras.Model(inputs, outputs)\n    \n    # Compile the model using Adam with a very low learning rate\n    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_LearningRate)\n    model.compile(\n        optimizer=optimizer,\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')]\n    )\n    \n    print(model.summary())\n\n########################################################################################################\n##########  CALLBACKS SETUP\n########################################################################################################\nSave_model_at = \"/kaggle/working/\"  # Adjust as needed\nModel_name = \"Classifier\"\nsave_model_after_each_epoch = ModelCheckpoint(\n    os.path.join(Save_model_at, f\"{Model_name}_Split_Part_{selected_split_part}_from_{num_split_parts}.h5\"),\n    verbose=1,\n    save_best_only=True\n)\n\nlr_reduction = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.1,\n    patience=Patience,\n    verbose=1,\n    min_delta=min_delta\n)\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=Patience, restore_best_weights=True)\n\n########################################################################################################\n##########  TRAIN THE MODEL\n########################################################################################################\nsteps_per_epoch = train_generator.samples // train_generator.batch_size\nvalidation_steps = validation_generator.samples // validation_generator.batch_size\n\nhistory = model.fit(\n    train_generator,\n    epochs=Epochs_number,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=validation_generator,\n    validation_steps=validation_steps,\n    class_weight=class_weights_dict,\n    callbacks=[save_model_after_each_epoch, lr_reduction, early_stop]\n)\n\n########################################################################################################\n##########  EVALUATION & PLOTTING\n########################################################################################################\ntest_loss, test_acc = model.evaluate(test_generator)\nprint(\"Test Accuracy: {:.2f}%\".format(test_acc * 100))\n\n# Plot Training & Validation Loss/Accuracy\nepochs_range = range(1, len(history.history['loss']) + 1)\nplt.figure(figsize=(12, 5))\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, history.history['loss'], 'bo-', label='Training Loss')\nplt.plot(epochs_range, history.history['val_loss'], 'ro-', label='Validation Loss')\nplt.title('Training vs Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, history.history['accuracy'], 'bo-', label='Training Accuracy')\nplt.plot(epochs_range, history.history['val_accuracy'], 'ro-', label='Validation Accuracy')\nplt.title('Training vs Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Plot Confusion Matrix\ny_true = test_generator.classes\ny_pred_probs = model.predict(test_generator)\ny_pred = np.argmax(y_pred_probs, axis=1)\ncm = confusion_matrix(y_true, y_pred)\nclass_names = list(test_generator.class_indices.keys())\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n# Plot ROC-AUC Curve for Multi-Class\nfrom sklearn.preprocessing import label_binarize\ny_true_bin = label_binarize(y_true, classes=np.arange(num_classes))\nplt.figure(figsize=(8, 6))\nfor i in range(num_classes):\n    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC-AUC Curve\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T19:44:38.977683Z","iopub.execute_input":"2025-02-21T19:44:38.978071Z","iopub.status.idle":"2025-02-21T19:44:49.473886Z","shell.execute_reply.started":"2025-02-21T19:44:38.978041Z","shell.execute_reply":"2025-02-21T19:44:49.472626Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.10/dist-packages (3.2.2)\nFound 168351 images belonging to 9 classes.\nFound 48098 images belonging to 9 classes.\nFound 24058 images belonging to 9 classes.\nGPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\nNumber of devices: 2\nClass weights: {0: 0.5766946191474493, 1: 0.48521871460316635, 2: 3.7188204108681244, 3: 0.8521167395529641, 4: 1.7156440123513406, 5: 2.986694342434403, 6: 1.639697288452548, 7: 1.1048828509549125, 8: 0.7518959187501675}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m4,049,571\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)                   │          \u001b[38;5;34m11,529\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)                   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,529</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,061,100\u001b[0m (15.49 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,061,100</span> (15.49 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,019,077\u001b[0m (15.33 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,019,077</span> (15.33 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m42,023\u001b[0m (164.16 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,023</span> (164.16 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-f504151454bb>\u001b[0m in \u001b[0;36m<cell line: 164>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0mSave_model_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/\"\u001b[0m  \u001b[0;31m# Adjust as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0mModel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m save_model_after_each_epoch = ModelCheckpoint(\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSave_model_at\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{Model_name}_Split_Part_{selected_split_part}_from_{num_split_parts}.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0;34m\"The filepath provided must end in `.keras` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0;34m\"(Keras model format). Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=/kaggle/working/Classifier_Split_Part_1_from_6.h5"],"ename":"ValueError","evalue":"The filepath provided must end in `.keras` (Keras model format). Received: filepath=/kaggle/working/Classifier_Split_Part_1_from_6.h5","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import os, random, shutil\nfrom glob import glob\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, save_img\n\n# Settings\nTARGET_COUNT = 20000\nSOURCE_DIR = \"/kaggle/working/organized_dataset/train\"  # Original training data folder (segmented images)\nBALANCED_DIR = \"/kaggle/working/balanced_dataset/train\"\n\nos.makedirs(BALANCED_DIR, exist_ok=True)\n\n# For each class folder in SOURCE_DIR, balance the number of images to exactly TARGET_COUNT\nfor class_name in os.listdir(SOURCE_DIR):\n    src_class_folder = os.path.join(SOURCE_DIR, class_name)\n    dst_class_folder = os.path.join(BALANCED_DIR, class_name)\n    os.makedirs(dst_class_folder, exist_ok=True)\n    \n    # List all image files (assuming PNG format; adjust if necessary)\n    images = glob(os.path.join(src_class_folder, \"*.png\"))\n    n = len(images)\n    print(f\"Class '{class_name}' has {n} images.\")\n    \n    if n >= TARGET_COUNT:\n        # Randomly sample TARGET_COUNT images\n        sampled = random.sample(images, TARGET_COUNT)\n        for img_path in sampled:\n            shutil.copy(img_path, dst_class_folder)\n    else:\n        # Copy all existing images\n        for img_path in images:\n            shutil.copy(img_path, dst_class_folder)\n        # Calculate how many augmented images are needed\n        needed = TARGET_COUNT - n\n        print(f\"Augmenting {needed} images for class '{class_name}'...\")\n        \n        # Define a mild augmentation generator\n        aug_datagen = ImageDataGenerator(\n            rotation_range=360,\n            vertical_flip=True,\n            horizontal_flip=True,\n            fill_mode='nearest'\n        )\n        \n        generated = 0\n        while generated < needed:\n            for img_path in images:\n                if generated >= needed:\n                    break\n                image = load_img(img_path, target_size=(256, 256))\n                x = img_to_array(image)\n                x = np.expand_dims(x, 0)\n                aug_iter = aug_datagen.flow(x, batch_size=1)\n                aug_image = next(aug_iter)[0]\n                # Save augmented image with a unique filename\n                save_path = os.path.join(dst_class_folder, f\"aug_{generated}_{os.path.basename(img_path)}\")\n                save_img(save_path, aug_image)\n                generated += 1\n        print(f\"Finished augmenting for class '{class_name}'.\")\n        \nprint(\"✅ Balanced dataset created at:\", BALANCED_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# print(tf.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Train class indices:\", train_generator.class_indices)\n# print(\"Number of training samples:\", train_generator.samples)\n# print(\"Number of validation samples:\", val_generator.samples)\n# print(\"Number of test samples:\", test_generator.samples)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for images, labels in train_generator:\n#     print(\"Batch shape:\", images.shape)\n#     print(\"Labels:\", labels)\n#     break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tensorflow as tf\n\n# Force TensorFlow to use GPU\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Enable memory growth for each GPU (prevents TensorFlow from allocating all memory at once)\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        # Optionally, force using only GPU 0 (if you have multiple GPUs and want to restrict usage)\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  \n        print(\"GPUs found:\", gpus)\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"No GPU found, running on CPU\")\n\n# Optionally, use a distribution strategy to automatically use all GPUs if available\nstrategy = tf.distribute.MirroredStrategy()\nprint(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import tensorflow as tf\n# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, ELU\n# from sklearn.utils import class_weight\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from sklearn.metrics import confusion_matrix, roc_curve, auc\n# from sklearn.preprocessing import label_binarize\n# from tensorflow.keras.layers import LeakyReLU\n# from tensorflow.keras.regularizers import l2\n\n# # Ensure GPU is configured (code above already executed)\n\n# # Define paths to your organized dataset folders\n# train_path = \"/kaggle/working/organized_dataset/train\"\n# val_path = \"/kaggle/working/organized_dataset/val\"\n# test_path = \"/kaggle/working/organized_dataset/test\"\n\n# # Parameters\n# IMG_SIZE = (256, 256)  # Using 256x256 images\n# BATCH_SIZE = 32\n# EPOCHS = 10\n\n# # Data generator: Only rescale (no augmentation)\n# datagen = ImageDataGenerator(rescale=1.0/255.0)\n\n# # Create generators from the dataset directories\n# train_generator = datagen.flow_from_directory(\n#     train_path,\n#     target_size=IMG_SIZE,\n#     batch_size=BATCH_SIZE,\n#     class_mode='categorical'\n# )\n\n# val_generator = datagen.flow_from_directory(\n#     val_path,\n#     target_size=IMG_SIZE,\n#     batch_size=BATCH_SIZE,\n#     class_mode='categorical'\n# )\n\n# test_generator = datagen.flow_from_directory(\n#     test_path,\n#     target_size=IMG_SIZE,\n#     batch_size=BATCH_SIZE,\n#     class_mode='categorical',\n#     shuffle=False  # For evaluation and metrics like confusion matrix\n# )\n\n# # Compute class weights using training labels\n# train_labels = train_generator.classes\n# unique_classes = np.unique(train_labels)\n# weights = class_weight.compute_class_weight(\n#     class_weight='balanced',\n#     classes=unique_classes,\n#     y=train_labels\n# )\n\n# class_weight_dict = dict(enumerate(weights))\n# print(\"Class weights:\", class_weight_dict)\n\n# # Build and train the model within the GPU strategy scope\n# with strategy.scope():\n#     num_classes = len(train_generator.class_indices)\n#     model = Sequential([\n#         Conv2D(32, (3, 3), input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n#         LeakyReLU(alpha=0.001),\n#         BatchNormalization(),\n#         MaxPooling2D((2, 2)),\n\n#         Conv2D(64, (3, 3)),\n#         LeakyReLU(alpha=0.001),\n#         BatchNormalization(),\n#         MaxPooling2D((2, 2)),\n\n#         # Conv2D(64, (3, 3)),\n#         # LeakyReLU(alpha=0.001),\n#         # BatchNormalization(),\n#         # MaxPooling2D((2, 2)),\n\n#         Flatten(),\n#         Dense((64),kernel_regularizer=l2(0.001)),\n#         LeakyReLU(alpha=0.01),\n#         BatchNormalization(),\n#         Dropout(0.5),\n#         Dense(num_classes, activation='softmax')\n#     ])\n\n#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n#     model.summary()\n\n# # Train the model using the computed class weights\n# history = model.fit(\n#     train_generator,\n#     epochs=EPOCHS,\n#     validation_data=val_generator,\n#     class_weight=class_weight_dict\n# )\n\n# # Evaluate the model on the test set\n# test_loss, test_acc = model.evaluate(test_generator)\n# print(\"Test Accuracy: {:.2f}%\".format(test_acc * 100))\n\n# # Save the model\n# model.save(\"/kaggle/working/rbc_classification_model.h5\")\n# print(\"Model saved to /kaggle/working/rbc_classification_model.h5\")\n\n# # -------------------------------------------\n# # Plot Training and Validation Loss/Accuracy\n# # -------------------------------------------\n# epochs_range = range(1, EPOCHS + 1)\n# plt.figure(figsize=(12, 5))\n# # Loss plot\n# plt.subplot(1, 2, 1)\n# plt.plot(epochs_range, history.history['loss'], 'bo-', label='Training Loss')\n# plt.plot(epochs_range, history.history['val_loss'], 'ro-', label='Validation Loss')\n# plt.title('Training vs Validation Loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n\n# # Accuracy plot\n# plt.subplot(1, 2, 2)\n# plt.plot(epochs_range, history.history['accuracy'], 'bo-', label='Training Accuracy')\n# plt.plot(epochs_range, history.history['val_accuracy'], 'ro-', label='Validation Accuracy')\n# plt.title('Training vs Validation Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# plt.show()\n\n# # -------------------------------------------\n# # Plot Confusion Matrix\n# # -------------------------------------------\n# y_true = test_generator.classes\n# y_pred_probs = model.predict(test_generator)\n# y_pred = np.argmax(y_pred_probs, axis=1)\n# cm = confusion_matrix(y_true, y_pred)\n# class_names = list(test_generator.class_indices.keys())\n\n# plt.figure(figsize=(8, 6))\n# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n# plt.xlabel(\"Predicted Label\")\n# plt.ylabel(\"True Label\")\n# plt.title(\"Confusion Matrix\")\n# plt.show()\n\n# # -------------------------------------------\n# # Plot ROC-AUC Curve\n# # -------------------------------------------\n# y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))\n# plt.figure(figsize=(8, 6))\n# for i in range(num_classes):\n#     fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])\n#     roc_auc = auc(fpr, tpr)\n#     plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n# plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n# plt.xlabel(\"False Positive Rate\")\n# plt.ylabel(\"True Positive Rate\")\n# plt.title(\"ROC-AUC Curve\")\n# plt.legend()\n# plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models, regularizers, constraints\nfrom tensorflow.keras.optimizers import Nadam\nfrom tensorflow.keras.layers import LeakyReLU, BatchNormalization, MaxPooling2D, Flatten, Dense, Conv2D, Dropout\nfrom tensorflow.keras.models import Sequential\n\n# Force GPU usage (ensure GPUs are visible and set memory growth)\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0 (adjust if needed)\n        print(\"GPUs configured:\", gpus)\n    except RuntimeError as e:\n        print(e)\n\n# Use MirroredStrategy to leverage multiple GPUs if available\nstrategy = tf.distribute.MirroredStrategy()\nprint(\"Number of devices:\", strategy.num_replicas_in_sync)\n\n# Define dataset paths for training (balanced), validation, and test\ntrain_path = \"/kaggle/working/balanced_dataset/train\"\nval_path = \"/kaggle/working/organized_dataset/val\"  # Assuming these are already organized\ntest_path = \"/kaggle/working/organized_dataset/test\"\n\n# Parameters\nIMG_SIZE = (80, 80)\nBATCH_SIZE = 64\nEPOCHS = 10\n\n# Data generators (only rescaling, no augmentation now)\ntrain_datagen = ImageDataGenerator(rescale=1.0/255.0)\nval_test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_path,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\nval_generator = val_test_datagen.flow_from_directory(\n    val_path,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\ntest_generator = val_test_datagen.flow_from_directory(\n    test_path,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Compute class weights using training labels (using keyword arguments)\nfrom sklearn.utils import class_weight\ntrain_labels = train_generator.classes\nunique_classes = np.unique(train_labels)\nweights = class_weight.compute_class_weight(class_weight='balanced', classes=unique_classes, y=train_labels)\nclass_weight_dict = dict(enumerate(weights))\nprint(\"Class weights:\", class_weight_dict)\n\n# Custom Monte Carlo Dropout layer (active during both training and inference)\nclass MCDropout(tf.keras.layers.Dropout):\n    def call(self, inputs, training=None):\n        return super().call(inputs, training=True)\n\n# Build the CNN model within the strategy scope\nwith strategy.scope():\n    num_classes = len(train_generator.class_indices)\n    model = Sequential([\n        Conv2D(32, (3, 3), padding='same', kernel_initializer='he_normal',\n               kernel_constraint=constraints.max_norm(3),\n               kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n               input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n        LeakyReLU(alpha=0.1),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n\n        Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal',\n               kernel_constraint=constraints.max_norm(3),\n               kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3)),\n        LeakyReLU(alpha=0.1),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n\n        Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal',\n               kernel_constraint=constraints.max_norm(3),\n               kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3)),\n        LeakyReLU(alpha=0.1),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n\n        Flatten(),\n        Dense(256, kernel_initializer='he_normal',\n              kernel_constraint=constraints.max_norm(3),\n              kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3)),\n        LeakyReLU(alpha=0.1),\n        BatchNormalization(),\n        MCDropout(0.5),  # Monte Carlo Dropout\n        Dense(num_classes, activation='softmax')\n    ])\n    optimizer = Nadam(learning_rate=0.000004)\n    # Compile using Nadam optimizer\n    model.compile(optimizer=Nadam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    model.summary()\n\n# Train the model using the computed class weights\nhistory = model.fit(\n    train_generator,\n    epochs=EPOCHS,\n    validation_data=val_generator,\n    class_weight=class_weight_dict\n)\n\n# Evaluate the model on the test set\ntest_loss, test_acc = model.evaluate(test_generator)\nprint(\"Test Accuracy: {:.2f}%\".format(test_acc * 100))\n\n# Save the model\nmodel.save(\"/kaggle/working/rbc_classification_model.h5\")\nprint(\"Model saved to /kaggle/working/rbc_classification_model.h5\")\n\n# -------------------------------------------\n# Plot Training and Validation Loss/Accuracy\n# -------------------------------------------\nimport matplotlib.pyplot as plt\nepochs_range = range(1, EPOCHS + 1)\nplt.figure(figsize=(12, 5))\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, history.history['loss'], 'bo-', label='Training Loss')\nplt.plot(epochs_range, history.history['val_loss'], 'ro-', label='Validation Loss')\nplt.title('Training vs Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, history.history['accuracy'], 'bo-', label='Training Accuracy')\nplt.plot(epochs_range, history.history['val_accuracy'], 'ro-', label='Validation Accuracy')\nplt.title('Training vs Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# -------------------------------------------\n# Plot Confusion Matrix\n# -------------------------------------------\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ny_true = test_generator.classes\ny_pred_probs = model.predict(test_generator)\ny_pred = np.argmax(y_pred_probs, axis=1)\ncm = confusion_matrix(y_true, y_pred)\nclass_names = list(test_generator.class_indices.keys())\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n# -------------------------------------------\n# Plot ROC-AUC Curve\n# -------------------------------------------\ny_true_bin = label_binarize(y_true, classes=np.arange(num_classes))\nplt.figure(figsize=(8, 6))\nfor i in range(num_classes):\n    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC-AUC Curve\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}